{"cells":[{"cell_type":"code","source":["fileName = \"/FileStore/tables/newcar.csv\"\n\nsplitIt =\",\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b7df691-355f-4d2e-b27f-8f3e14236ba2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["fileLines = sc.textFile(fileName)\nheader=fileLines.first()\n\nprint(fileLines.collect())\n\nrawdata=fileLines.filter(lambda x: x!=header)\n\nprint(rawdata.count())\ncarDataIs=rawdata.take(rawdata.count())\nprint(carDataIs)\nrawdata.persist()\nrawdata.cache()\n#rawdata.saveAsTextFile(\"/FileStore/tables/carsdata.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f884d9a-0a3b-4dc5-907b-bbe1988cff05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['make,Model,Milespg,Cylinders,Engine_Disp,Horsepower,Weight,Accelerate,Year,Origin', 'bmw,bmw 2002,26,4,161,112,2233,12.5,70,European', 'peugeot,peugeot 504,25,4,110,88,2672,17.5,70,European', 'saab,saab 99e,25,4,104,95,2375,17.5,70,European', 'datsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese', 'toyato,toyota corona,25,4,113,95,2229,14,71,Japanese', 'amc,amc gremlin,19,6,232,100,2634,13,71,American', 'amc,amc hornet,18,6,258,110,2962,13.5,71,American']\n7\n['bmw,bmw 2002,26,4,161,112,2233,12.5,70,European', 'peugeot,peugeot 504,25,4,110,88,2672,17.5,70,European', 'saab,saab 99e,25,4,104,95,2375,17.5,70,European', 'datsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese', 'toyato,toyota corona,25,4,113,95,2229,14,71,Japanese', 'amc,amc gremlin,19,6,232,100,2634,13,71,American', 'amc,amc hornet,18,6,258,110,2962,13.5,71,American']\nOut[12]: PythonRDD[74] at RDD at PythonRDD.scala:58","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['make,Model,Milespg,Cylinders,Engine_Disp,Horsepower,Weight,Accelerate,Year,Origin', 'bmw,bmw 2002,26,4,161,112,2233,12.5,70,European', 'peugeot,peugeot 504,25,4,110,88,2672,17.5,70,European', 'saab,saab 99e,25,4,104,95,2375,17.5,70,European', 'datsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese', 'toyato,toyota corona,25,4,113,95,2229,14,71,Japanese', 'amc,amc gremlin,19,6,232,100,2634,13,71,American', 'amc,amc hornet,18,6,258,110,2962,13.5,71,American']\n7\n['bmw,bmw 2002,26,4,161,112,2233,12.5,70,European', 'peugeot,peugeot 504,25,4,110,88,2672,17.5,70,European', 'saab,saab 99e,25,4,104,95,2375,17.5,70,European', 'datsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese', 'toyato,toyota corona,25,4,113,95,2229,14,71,Japanese', 'amc,amc gremlin,19,6,232,100,2634,13,71,American', 'amc,amc hornet,18,6,258,110,2962,13.5,71,American']\nOut[12]: PythonRDD[74] at RDD at PythonRDD.scala:58"]}}],"execution_count":0},{"cell_type":"code","source":["print(header)\nfor(rowIs) in carDataIs:\n  print(\"%s\" % (rowIs))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abdddb91-e4b1-4bb4-b55f-87a9b8d97a6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"make,Model,Milespg,Cylinders,Engine_Disp,Horsepower,Weight,Accelerate,Year,Origin\nbmw,bmw 2002,26,4,161,112,2233,12.5,70,European\npeugeot,peugeot 504,25,4,110,88,2672,17.5,70,European\nsaab,saab 99e,25,4,104,95,2375,17.5,70,European\ndatsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese\ntoyato,toyota corona,25,4,113,95,2229,14,71,Japanese\namc,amc gremlin,19,6,232,100,2634,13,71,American\namc,amc hornet,18,6,258,110,2962,13.5,71,American\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["make,Model,Milespg,Cylinders,Engine_Disp,Horsepower,Weight,Accelerate,Year,Origin\nbmw,bmw 2002,26,4,161,112,2233,12.5,70,European\npeugeot,peugeot 504,25,4,110,88,2672,17.5,70,European\nsaab,saab 99e,25,4,104,95,2375,17.5,70,European\ndatsun,datsun pl510,27,4,97,88,2130,14.5,70,Japanese\ntoyato,toyota corona,25,4,113,95,2229,14,71,Japanese\namc,amc gremlin,19,6,232,100,2634,13,71,American\namc,amc hornet,18,6,258,110,2962,13.5,71,American\n"]}}],"execution_count":0},{"cell_type":"code","source":["carSplitData = rawdata.map(lambda x : x.split(splitIt))\n\nprint(\"Data Split By Delimiter\" , carSplitData.collect())\n\nprint(\"\\n\\nMake\\tModel\\t\\tMPG\\tCylinders\\tEngine_Disp\\tHorsepower\\tWeight\\tAcceleration\\tYear\\tOrigin\")\nfor(carData) in carSplitData.collect():\n  print(\"%s\\t%s\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t%s\\t\\t%s\\t%s\" % (carData[0],carData[1],carData[2],carData[3],carData[4],carData[5],carData[6],carData[7],carData[8],carData[9]))\n\n\ncarColumns = [\"Make\",\"Model\",\"MPG\",\"Cylinders\",\"Engine_Disp\",\"Horsepower\",\"Weight\",\"Acceleration\",\"Year\",\"Origin\"]\ndfIs=carSplitData.toDF(carColumns)\ndisplay(dfIs)\ndfIs.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b1ecbc9-8331-42ba-994e-978d4a878c8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Data Split By Delimiter [['bmw', 'bmw 2002', '26', '4', '161', '112', '2233', '12.5', '70', 'European'], ['peugeot', 'peugeot 504', '25', '4', '110', '88', '2672', '17.5', '70', 'European'], ['saab', 'saab 99e', '25', '4', '104', '95', '2375', '17.5', '70', 'European'], ['datsun', 'datsun pl510', '27', '4', '97', '88', '2130', '14.5', '70', 'Japanese'], ['toyato', 'toyota corona', '25', '4', '113', '95', '2229', '14', '71', 'Japanese'], ['amc', 'amc gremlin', '19', '6', '232', '100', '2634', '13', '71', 'American'], ['amc', 'amc hornet', '18', '6', '258', '110', '2962', '13.5', '71', 'American']]\n\n\nMake\tModel\t\tMPG\tCylinders\tEngine_Disp\tHorsepower\tWeight\tAcceleration\tYear\tOrigin\nbmw\tbmw 2002\t26\t4\t\t161\t\t112\t\t2233\t12.5\t\t70\tEuropean\npeugeot\tpeugeot 504\t25\t4\t\t110\t\t88\t\t2672\t17.5\t\t70\tEuropean\nsaab\tsaab 99e\t25\t4\t\t104\t\t95\t\t2375\t17.5\t\t70\tEuropean\ndatsun\tdatsun pl510\t27\t4\t\t97\t\t88\t\t2130\t14.5\t\t70\tJapanese\ntoyato\ttoyota corona\t25\t4\t\t113\t\t95\t\t2229\t14\t\t71\tJapanese\namc\tamc gremlin\t19\t6\t\t232\t\t100\t\t2634\t13\t\t71\tAmerican\namc\tamc hornet\t18\t6\t\t258\t\t110\t\t2962\t13.5\t\t71\tAmerican\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Data Split By Delimiter [['bmw', 'bmw 2002', '26', '4', '161', '112', '2233', '12.5', '70', 'European'], ['peugeot', 'peugeot 504', '25', '4', '110', '88', '2672', '17.5', '70', 'European'], ['saab', 'saab 99e', '25', '4', '104', '95', '2375', '17.5', '70', 'European'], ['datsun', 'datsun pl510', '27', '4', '97', '88', '2130', '14.5', '70', 'Japanese'], ['toyato', 'toyota corona', '25', '4', '113', '95', '2229', '14', '71', 'Japanese'], ['amc', 'amc gremlin', '19', '6', '232', '100', '2634', '13', '71', 'American'], ['amc', 'amc hornet', '18', '6', '258', '110', '2962', '13.5', '71', 'American']]\n\n\nMake\tModel\t\tMPG\tCylinders\tEngine_Disp\tHorsepower\tWeight\tAcceleration\tYear\tOrigin\nbmw\tbmw 2002\t26\t4\t\t161\t\t112\t\t2233\t12.5\t\t70\tEuropean\npeugeot\tpeugeot 504\t25\t4\t\t110\t\t88\t\t2672\t17.5\t\t70\tEuropean\nsaab\tsaab 99e\t25\t4\t\t104\t\t95\t\t2375\t17.5\t\t70\tEuropean\ndatsun\tdatsun pl510\t27\t4\t\t97\t\t88\t\t2130\t14.5\t\t70\tJapanese\ntoyato\ttoyota corona\t25\t4\t\t113\t\t95\t\t2229\t14\t\t71\tJapanese\namc\tamc gremlin\t19\t6\t\t232\t\t100\t\t2634\t13\t\t71\tAmerican\namc\tamc hornet\t18\t6\t\t258\t\t110\t\t2962\t13.5\t\t71\tAmerican\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["bmw","bmw 2002","26","4","161","112","2233","12.5","70","European"],["peugeot","peugeot 504","25","4","110","88","2672","17.5","70","European"],["saab","saab 99e","25","4","104","95","2375","17.5","70","European"],["datsun","datsun pl510","27","4","97","88","2130","14.5","70","Japanese"],["toyato","toyota corona","25","4","113","95","2229","14","71","Japanese"],["amc","amc gremlin","19","6","232","100","2634","13","71","American"],["amc","amc hornet","18","6","258","110","2962","13.5","71","American"]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}]},"pivotColumns":[],"pivotAggregation":"sum","xColumns":["Origin"],"yColumns":["Weight"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Make","type":"\"string\"","metadata":"{}"},{"name":"Model","type":"\"string\"","metadata":"{}"},{"name":"MPG","type":"\"string\"","metadata":"{}"},{"name":"Cylinders","type":"\"string\"","metadata":"{}"},{"name":"Engine_Disp","type":"\"string\"","metadata":"{}"},{"name":"Horsepower","type":"\"string\"","metadata":"{}"},{"name":"Weight","type":"\"string\"","metadata":"{}"},{"name":"Acceleration","type":"\"string\"","metadata":"{}"},{"name":"Year","type":"\"string\"","metadata":"{}"},{"name":"Origin","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Make</th><th>Model</th><th>MPG</th><th>Cylinders</th><th>Engine_Disp</th><th>Horsepower</th><th>Weight</th><th>Acceleration</th><th>Year</th><th>Origin</th></tr></thead><tbody><tr><td>bmw</td><td>bmw 2002</td><td>26</td><td>4</td><td>161</td><td>112</td><td>2233</td><td>12.5</td><td>70</td><td>European</td></tr><tr><td>peugeot</td><td>peugeot 504</td><td>25</td><td>4</td><td>110</td><td>88</td><td>2672</td><td>17.5</td><td>70</td><td>European</td></tr><tr><td>saab</td><td>saab 99e</td><td>25</td><td>4</td><td>104</td><td>95</td><td>2375</td><td>17.5</td><td>70</td><td>European</td></tr><tr><td>datsun</td><td>datsun pl510</td><td>27</td><td>4</td><td>97</td><td>88</td><td>2130</td><td>14.5</td><td>70</td><td>Japanese</td></tr><tr><td>toyato</td><td>toyota corona</td><td>25</td><td>4</td><td>113</td><td>95</td><td>2229</td><td>14</td><td>71</td><td>Japanese</td></tr><tr><td>amc</td><td>amc gremlin</td><td>19</td><td>6</td><td>232</td><td>100</td><td>2634</td><td>13</td><td>71</td><td>American</td></tr><tr><td>amc</td><td>amc hornet</td><td>18</td><td>6</td><td>258</td><td>110</td><td>2962</td><td>13.5</td><td>71</td><td>American</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Make: string (nullable = true)\n |-- Model: string (nullable = true)\n |-- MPG: string (nullable = true)\n |-- Cylinders: string (nullable = true)\n |-- Engine_Disp: string (nullable = true)\n |-- Horsepower: string (nullable = true)\n |-- Weight: string (nullable = true)\n |-- Acceleration: string (nullable = true)\n |-- Year: string (nullable = true)\n |-- Origin: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Make: string (nullable = true)\n |-- Model: string (nullable = true)\n |-- MPG: string (nullable = true)\n |-- Cylinders: string (nullable = true)\n |-- Engine_Disp: string (nullable = true)\n |-- Horsepower: string (nullable = true)\n |-- Weight: string (nullable = true)\n |-- Acceleration: string (nullable = true)\n |-- Year: string (nullable = true)\n |-- Origin: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#get all horsepower data\nfrom functools import reduce\nfrom operator import add\n\ncarRows = rawdata.map(lambda x:x.split(splitIt)[5]).map(int)\nprint(\"Horsepower Data\\n\", carRows.collect())\n\n\nprint(\"Sum of Horsepower Data: \", reduce(add, carRows.collect()))\n\nprint(carRows.sum())\n\nprint(\"Total of Horsepower : \", carRows.map(lambda x:x).sum(), \"hp\")\n\nprint(\"Average of Horsepower Data {:.2f}\".format(reduce(add, carRows.collect())/carRows.count()), \"hp\")\nprint(\"Average of Horsepower Data {:.2f}\".format(carRows.mean()), \"hp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43bcd955-e5ed-4175-a3ef-a9d698e6138d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Horsepower Data\n [112, 88, 95, 88, 95, 100, 110]\nSum of Horsepower Data:  688\n688\nTotal of Horsepower :  688 hp\nAverage of Horsepower Data 98.29 hp\nAverage of Horsepower Data 98.29 hp\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Horsepower Data\n [112, 88, 95, 88, 95, 100, 110]\nSum of Horsepower Data:  688\n688\nTotal of Horsepower :  688 hp\nAverage of Horsepower Data 98.29 hp\nAverage of Horsepower Data 98.29 hp\n"]}}],"execution_count":0},{"cell_type":"code","source":["#get sum and mean of  weights\n\ncarRows = rawdata.map(lambda x:x.split(splitIt)[6]).map(int)\n\nprint(\"Weight Data\\n\", carRows.collect())\nprint(\"Sum of Cars Weights: \", carRows.sum(), \"kg\")\nprint(\"Average of Car Weights: {:.2f}\".format(carRows.mean()),\"kg\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1083aaa3-deb9-44dd-91ea-8a539faca6b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Weight Data\n [2233, 2672, 2375, 2130, 2229, 2634, 2962]\nSum of Cars Weights:  17235 kg\nAverage of Car Weights: 2462.14 kg\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Weight Data\n [2233, 2672, 2375, 2130, 2229, 2634, 2962]\nSum of Cars Weights:  17235 kg\nAverage of Car Weights: 2462.14 kg\n"]}}],"execution_count":0},{"cell_type":"code","source":["#filter out by weight car weight > 2300\n\nfilterVal = 2300\n\ncarWeightIs = carRows.filter(lambda x: int(x[6]) > filterVal)\n\n#print(carWeightIs.collect())\n\nprint(\"\\n\\nMake\\tModel\\t\\tMPG\\tCylinders\\tEngine_Disp\\tHorsepower\\tWeight\\tAcceleration\\tYear\\tOrigin\")\nfor(carData) in carWeightIs.collect():\n  print(\"%s\\t%s\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t%s\\t\\t%s\\t%s\" % (carData[0],carData[1],carData[2],carData[3],carData[4],carData[5],carData[6],carData[7],carData[8],carData[9]))\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46dd2d99-8998-4c03-8af7-f9e3528f2e62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n\nMake\tModel\t\tMPG\tCylinders\tEngine_Disp\tHorsepower\tWeight\tAcceleration\tYear\tOrigin\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n\nMake\tModel\t\tMPG\tCylinders\tEngine_Disp\tHorsepower\tWeight\tAcceleration\tYear\tOrigin\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1699807260258947>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n\\nMake\\tModel\\t\\tMPG\\tCylinders\\tEngine_Disp\\tHorsepower\\tWeight\\tAcceleration\\tYear\\tOrigin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcarWeightIs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s\\t%s\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t%s\\t\\t%s\\t%s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m9\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    964\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 966\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    967\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 40) (ip-10-172-229-66.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable', from <command-1699807260258947>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 770, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 762, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-1699807260258947>\", line 5, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2651)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable', from <command-1699807260258947>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 770, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 762, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-1699807260258947>\", line 5, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 40) (ip-10-172-229-66.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable', from <command-1699807260258947>, line 5. Full traceback below:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1699807260258947>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n\\nMake\\tModel\\t\\tMPG\\tCylinders\\tEngine_Disp\\tHorsepower\\tWeight\\tAcceleration\\tYear\\tOrigin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcarWeightIs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s\\t%s\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t%s\\t\\t%s\\t%s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcarData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m9\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    964\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 966\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    967\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 40) (ip-10-172-229-66.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable', from <command-1699807260258947>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 770, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 762, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-1699807260258947>\", line 5, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2651)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable', from <command-1699807260258947>, line 5. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 770, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 762, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-1699807260258947>\", line 5, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["#filter by origin\n\ncarOrigin =\"European\"\n\ncarRows=rawdata.map(lambda x: x.split(splitIt)).filter(lambda x: x[9]==carOrigin)\n\nprint(\"\\n\\nMake\\tModel\\t\\tYear\\tOrigin\")\nfor(carData) in carRows.collect():\n  print(\"%s\\t%s\\t19%s\\t%s\" % (carData[0],carData[1],carData[8],carData[9]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f12149dd-7eaf-40f4-af4a-7f408939ac7a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n\nMake\tModel\t\tYear\tOrigin\nbmw\tbmw 2002\t1970\tEuropean\npeugeot\tpeugeot 504\t1970\tEuropean\nsaab\tsaab 99e\t1970\tEuropean\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n\nMake\tModel\t\tYear\tOrigin\nbmw\tbmw 2002\t1970\tEuropean\npeugeot\tpeugeot 504\t1970\tEuropean\nsaab\tsaab 99e\t1970\tEuropean\n"]}}],"execution_count":0},{"cell_type":"code","source":["#calculate sum and average of the weight of car by it's origin\n\noriginWeightSum=rawdata.map(lambda x:x.split(splitIt)).map(lambda x:(x[9], int(x[6])))\\\n    .combineByKey(lambda x: (x, 1),\\\n                  lambda a, x:(a[0] + x, a[1] +1),\\\n                  lambda a, x:(a[0] + x[0], a[1] + x[1]))\n\nprint(\"Origin\\t\\tSum Of Cars Weight(kg)\\t\\tOccurance\")\nfor(origin, (sumWeight, occurs)) in originWeightSum.collect():\n    print(\"%s\\t\\t%i\\t\\t\\t%i\" % (origin, sumWeight, occurs))\n\noriginWeightAvg=originWeightSum.map(lambda x: (x[0], (x[1][0]/x[1][1])))\nprint(\"\\n\\nOrigin\\t\\tAverate Weight(kg)\")\nfor(origin, mean) in originWeightAvg.collect():\n  print(\"%s\\t\\t%f\" % (origin, mean))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"205b4148-5068-4070-9b0b-0e35284c6983"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Origin\t\tSum Of Cars Weight(kg)\t\tOccurance\nEuropean\t\t7280\t\t\t3\nAmerican\t\t5596\t\t\t2\nJapanese\t\t4359\t\t\t2\n\n\nOrigin\t\tAverate Weight(kg)\nEuropean\t\t2426.666667\nAmerican\t\t2798.000000\nJapanese\t\t2179.500000\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Origin\t\tSum Of Cars Weight(kg)\t\tOccurance\nEuropean\t\t7280\t\t\t3\nAmerican\t\t5596\t\t\t2\nJapanese\t\t4359\t\t\t2\n\n\nOrigin\t\tAverate Weight(kg)\nEuropean\t\t2426.666667\nAmerican\t\t2798.000000\nJapanese\t\t2179.500000\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CarDataRDD","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2853949933450099}},"nbformat":4,"nbformat_minor":0}
